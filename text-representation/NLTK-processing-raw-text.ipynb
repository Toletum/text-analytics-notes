{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from urllib import urlopen\n",
    "url = \"http://www.gutenberg.org/files/2554/2554.txt\"\n",
    "raw = urlopen(url).read()\n",
    "raw[:75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization \n",
    "the process to break down text into basic units - words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = word_tokenize(raw)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using NLTK text object\n",
    "You can convert a list of tokens into a Text object and use the functions there "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = nltk.Text(tokens)\n",
    "text.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BeautifulSoup to extract relevant content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html= urlopen(url).read().decode('utf8')\n",
    "raw = BeautifulSoup(html).get_text()\n",
    "tokens = word_tokenize(raw)\n",
    "\n",
    "text = nltk.Text(tokens[110:390]) # manually filter menu\n",
    "text.concordance('gene')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using FeedParser to access am Atom feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import feedparser\n",
    "\n",
    "llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")\n",
    "llog['feed']['title']\n",
    "len(llog.entries)\n",
    "post = llog.entries[2]\n",
    "post.title\n",
    "content = post.content[0].value\n",
    "content[0:70]\n",
    "word_tokenize(BeautifulSoup(content).get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Unicode\n",
    "Python 3 sourcecode is byh default in UTF-8. Codecs is not needed as it has support for encoding in open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "path = nltk.data.find('corpora/unicode_samples/polish-lat2.txt')\n",
    "f = codecs.open(path, encoding='latin2')\n",
    "\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    print line.encode('unicode_escape')\n",
    "    print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with regular expressions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[w for w in wordlist if re.search('ed$',w)] # Find words ending in -ed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "     is no basis for a system of government.  Supreme executive power derives from \n",
    "     a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "    \n",
    "tokens = nltk.word_tokenize(raw)\n",
    "print tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "stems = [porter.stem(t) for t in tokens]\n",
    "print stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lancaster = nltk.LancasterStemmer()\n",
    "stems = [lancaster.stem(t) for t in tokens]\n",
    "print stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "lemmas = [wnl.lemmatize(t) for t in tokens]\n",
    "print lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')\n",
    "sents = sent_tokenizer.tokenize(text)\n",
    "pprint.pprint(sents[171:181])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
